
ğŸ› ï¸ Etapas Realizadas
âš™ï¸ 1. Ambiente e SessÃ£o PySpark
InstalaÃ§Ã£o do PySpark via pip.

ConfiguraÃ§Ã£o de variÃ¡veis de ambiente para compatibilidade com o Python.

CriaÃ§Ã£o da SparkSession com o nome PySpark_01.

ğŸ“‚ 2. Leitura do Dataset
Leitura do arquivo .csv com spark.read.csv(), utilizando:

header=True para reconhecer cabeÃ§alhos.

inferSchema=True para inferÃªncia automÃ¡tica dos tipos.

ExibiÃ§Ã£o inicial dos dados com df.show() e estrutura com df.printSchema().

ğŸ§ª 3. AnÃ¡lise de Dados Nulos
ConversÃ£o para DataFrame Pandas: df.toPandas().isna().sum() para detectar colunas com valores ausentes.

VerificaÃ§Ã£o com PySpark usando filter() e count() para checar linhas nulas diretamente no ambiente distribuÃ­do.

ğŸ§¹ 4. Tratamento de Dados
RenomeaÃ§Ã£o de colunas com caracteres especiais, facilitando manipulaÃ§Ã£o (ex: Pos. â†’ Posicao, # â†’ Numero, Club â†’ Time).

PadronizaÃ§Ã£o para nomes descritivos em snake_case.

